# Day 3: Wednesday August 22

### General Topics

- Introduction to Machine Learning and Data Mining
  - Machine Learning basic concepts
  - Taxonomy of ML algorithms
    - Supervised classification,
    - Unsupervised classification,
    - Semi-supervised classification
  - Clustering validation and cross validation 

- Applications of ML in Bioinformatics
  - Examples of different ML/DM techniques that can be applied to different NGS data analysis pipelines
  - How to chose the right ML technique?

  
- Deep learning quick overview

- Exploring some examples with R ML packages

## Schedule

- _**09:00 - 10:00**_ Introduction to  DM and ML, Machine Learning basic concepts

- _**10:00 - 11:00**_ Taxonomy of ML and examples of algorithms
    - Supervised classification,
    - Unsupervised classification,
    - Semi-supervised classification

- _**11:00 - 11:30**_ _**Coffee break**_

- _**11:30 - 12:30**_ Applications of ML in Bioinformatics
    - Examples of different ML/DM techniques that can be applied to different NGS data analysis pipelines
    - How to chose the right ML technique?
    - Deep learning overview

- _**12:30 - 14:00**_ _**Lunch break**_

- _**14:00 - 15:00**_ We will start practicing using the built-in R [data set iris](https://archive.ics.uci.edu/ml/datasets/iris)

- _**15:00 - 16:00**_ RNASeq analysis using clustering in R <br/>
[Example we will be using during the session](http://www.2-bitbio.com/2017/04/clustering-rnaseq-data-making-heatmaps.html)<br/>



- _**16:00 - 16:15**_ _**Coffee break**_

- _**16:15 - 18:00**_ RNASeq analysis in R to be continued

RNASeq analysis in R <br/>
[Differential expression analysis using R](https://combine-australia.github.io/RNAseq-R/06-rnaseq-day1.html) <br/>
Download Mouse mammary RNA-Seq data (counts) data files from [this public repo](https://figshare.com/s/1d788fd384d33e913a2a)<br/>

## Let's start doing some clustering on the IRIS dataset
To check all the R Built-in data sets, you can use the command:
```
data() #it will display all the available default datasets 
```
```
iris #to check the dataset
```
Preprocessing the data: exclude column "Species" at position 5 and Standardize

```
F_iris <- iris[,-5] # Remove column 5
F_iris <- scale(F_iris) # Standardize: Scale is a generic function whose default method centers and/or scales the columns of a numeric matrix.
```

Let's cluster the data using function eclust() (enhanced clustering, in factoextra). You will need to install factoextra package first.<br/> 

Install Bioconductor packages. More details [here](https://www.bioconductor.org/install/) <br/>

```
source("https://bioconductor.org/biocLite.R")
biocLite()
```
Install factoextra
```
biocLite("factoextra")
```
Load factoextra
```
library(factoextra)
```

For more info about the eclust() function, check [this](https://www.rdocumentation.org/packages/factoextra/versions/1.0.5/topics/eclust)
or type
```
help(eclust)
eclust(x, FUNcluster = "kmeans", hc_metric = "euclidean", ...) 
```
Now let's try to cluster the data with Kmeans and then use hierarchical clustering 

```
eclust(F_iris, FUNcluster = "kmeans", hc_metric = "euclidean") #number of clusters not specified, will be detected automatically
# K-means clustering giving number of clusters= 3
km.res <- eclust(df, "kmeans", k = 3, nstart = 25, graph = FALSE)
# Visualize k-means clusters
fviz_cluster(km.res, geom = "point", ellipse.type = "norm", palette = "jco", ggtheme = theme_minimal())
```

```
hc.res <- eclust(F_iris, "hclust", k = 3, hc_metric = "euclidean", hc_method = "ward.D2", graph = FALSE)
# Visualize dendrograms
fviz_dend(hc.res, show_labels = FALSE, palette = "jco", as.ggplot = TRUE)
 ```
Let's check the clusters quality, remember we talked this morning about clustering internal validation and the use of coefficients and indexes to measure clusters separability and intra clusters homogenity. We will compute the silhouette coefficient for the resulting K-means clustering

```
fviz_silhouette(km.res, palette = "jco", ggtheme = theme_classic())
```
A value close to 1 indicates good quality clustering (element assigned to the right cluster)<br/>
A value close to -1 indicates poor quality clustering (element would better fit in another cluster)<br/>

Now try to vary the number of clusters, run Kmeans and compute the the silhouette coefficient. What did you notice? <br/>

Many other coefficients could be computed to assess the clustering quality, some are available using the cluster.stats() function of the fpc package. You might need to install the fpc package first.
```
biocLite("fpc")
library(fpc)
kmIris_stats <- cluster.stats(dist(F_iris),  km.res$cluster)
kmIris_stats$dunn # if you wanna see only the dunn index value
```
Remember, iris dataset is labelled and we removed labels before performing clustering. You can perform a cross-tabulation between k-means clusters and the reference Species labels using table.
```
table(iris$Species, km.res$cluster)
```
The nbclust() function of the fviz package give an estimate of the optimal number of clusters, let's try to estimate the number of clusters for the iris dataset based on the use of k-means clustering:
```
fviz_nbclust(F_iris, kmeans,method = "gap_stat")
```
Take few minutes to check the different functions of the [Cluster package](https://cran.r-project.org/web/packages/cluster/cluster.pdf)<br/>.

## Let's try some supervised classification on the IRIS dataset
Remeber the iris dataset is a labelled one (the classes are known) but we removed labels in the previous exercice to perform clustering. <br/>
First We will need the caret package to be installed, 
```
install.packages("caret", dependencies=c("Depends", "Suggests")) #installation of the caret packages and all packages that you might need 
library(caret)
```

Let's load the dataset from the CSV file as follows (useful when working with non R Built-in datasets):<br/>

```
wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data #the file iris.data will be downloaded in your current working directory, make sure you either specify the path while reading the file with R or copy it to your default R working directory
#backtoR
getwd()#check the default working directory
setwd(path) #to change the working directory
irisfile <- "iris.data"
# load the CSV file from the local directory
irisData <- read.csv(irisfile, header=FALSE)
# set the column names in the dataset
colnames(irisData) <- c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species")
```
All the iris data is labelled, to allow testing the model, let's create a validation (test) sub-dataset. We will split the loaded dataset into two, 80% of which we will use to train our models and 20% that we will hold back as a validation dataset.

```
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(irisData$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- irisData[-validation_index,]
# use the remaining 80% of data to training and testing the models
irisData <- irisData[validation_index,]
# you can check the dimensions of dataset
dim(irisData)
# list types for each attribute
sapply(irisData, class) # all the variables are numeric and the class value is a factor
head(irisData) #if you want to check the first 5 lines of the dataset
tail(irisData) #if you want to check the last 5 lines of the dataset
```
You now have training data in the irisData variable and a validation set we will use later in the validation variable.

## Some interesting readings: application of ML in Bioinformatics

[A machine learning model to determine the accuracy of variant calls in capture-based next generation sequencing](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-4659-0) <br/>
[Next-Generation Machine Learning for Biological Networks](https://www.ncbi.nlm.nih.gov/pubmed/29887378) <br/>
[Machine learning for metagenomics: methods and tools](https://arxiv.org/pdf/1510.06621.pdf) <br/>

## Some useful R/Bioconductor packages to handle Bioinformatics files and use ML algorithms

[Cluster package](https://cran.r-project.org/web/packages/cluster/cluster.pdf)<br/>
[vcfR, a package to read, manipulate and analyze VCF files](https://knausb.github.io/vcfR_documentation/index.html)<br/>
[Practical Guide To Cluster Analysis in R](https://goo.gl/13EFCZ)<br/>


### Lead Instructor
- [Amel Ghouila](https://github.com/amelgh) | [@AmelGhouila](https://twitter.com/AmelGhouila) | [email](mailto:amel.ghouila@gmail.com)

### Co-Instructor(s)
- [Fotis Psomopoulos](https://fpsom.github.io/) | [@fopsom](https://twitter.com/fopsom) | [email](mailto:fopsom@gmail.com)

### Helper(s)
- Maria Tsagiopoulou | [email](mariatsayo@gmail.com)
- Ola Karrar | [email](ozkarrar2@gmail.com)

Sources used:
[http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/#data-preparation](http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/#data-preparation)
[https://machinelearningmastery.com/machine-learning-in-r-step-by-step/](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/)

